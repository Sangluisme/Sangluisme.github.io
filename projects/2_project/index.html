<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Erasing the Ephemeral | Lu Sang </title> <meta name="author" content="Lu Sang"> <meta name="description" content="Joint Camera Refinement and Transient Object Removal for Street View Synthesis"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.jpg?1e4f83a7c4e3dbb8f1584da79979eb93"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sangluisme.github.io/projects/2_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Erasing the Ephemeral",
            "description": "Joint Camera Refinement and Transient Object Removal for Street View Synthesis",
            "published": "October 06, 2024",
            "authors": [
              
              {
                "author": "Mreenav Deka*",
                "authorURL": "https://github.com/Dawars",
                "affiliations": [
                  {
                    "name": "TUM",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Lu Sang*",
                "authorURL": "https://sangluisme.github.io",
                "affiliations": [
                  {
                    "name": "TUM, MCML",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Daniel Cremers",
                "authorURL": "https://cvg.cit.tum.de/members/cremers",
                "affiliations": [
                  {
                    "name": "TUM, MCML",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Lu</span> Sang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Erasing the Ephemeral</h1> <p>Joint Camera Refinement and Transient Object Removal for Street View Synthesis</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#abstract">Abstract</a> </div> <div> <a href="#overview">Overview</a> </div> <div> <a href="#results">Results</a> </div> <ul> <li> <a href="#novel-view-synthesis">Novel view synthesis</a> </li> <li> <a href="#trajectory-extrapolation">Trajectory extrapolation</a> </li> </ul> <div> <a href="#citation">Citation</a> </div> </nav> </d-contents> <video width="100%" autoplay="" muted="" loop=""> <source src="/assets/img/street/street.mp4" type="video/mp4"></source> </video> <p><em><strong>Reconstructed views on Waymo.</strong> Reconstructed scenes of a sequence from Waymo dataset. Our method can eliminate moving objects on the street.</em></p> <h2 id="abstract">Abstract</h2> <p>Synthesizing novel views for urban environments is crucial for tasks like autonomous driving and virtual tours. Compared to object-level or indoor situations, outdoor settings present unique challenges such as inconsistency across frames due to moving vehicles and camera pose drift over lengthy sequences. In this paper, we introduce a method that tackles these challenges on view synthesis for outdoor scenarios. We employ a neural point light field scene representation and strategically detect and mask out dynamic objects to reconstruct novel scenes without artifacts. Moreover, we simultaneously optimize camera pose along with the view synthesis process, and thus we simultaneously refine both elements. Through validation on real-world urban datasets, we demonstrate state-of-the-art results in synthesizing novel views of urban scenes.</p> <h2 id="overview">Overview</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/street/teaser.png" sizes="95vw"></source> <img src="/assets/img/street/teaser.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>a) We incorporate novel view synthesis with dynamic object erasing which removes artifacts created by inconsistent frames in urban scenes.</p> <p>b) We propose a voting scheme for dynamic object detection to achieve consistent classification of moving objects.</p> <p>c) During training, we jointly refine camera poses and demonstrate the robustness of our method to substantial camera pose noise. As a result, image quality is elevated with the increased accuracy of camera poses.</p> <h3 id="moving-object-detection">Moving Object Detection</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/street/object_detection.png" sizes="95vw"></source> <img src="/assets/img/street/object_detection.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <strong>Moving object detection</strong>. Comparison with and without voting scheme.* </div> <p>we employ a voting scheme to reduce inconsistencies in motion prediction that may be caused by incorrect optical field computation or the inconsistencies introduced by ego-motion. In frame $j$ where the object with instance $i$ appears, we compute the motion score \(m_j^i\in \{0,1\}\), where 1 and 0 denote moving and non-moving objects respectively. Thus, each object has a sequence of motion labels \(\{m^i_n\}_n\) (out side $n$ means iterate over \(n\)) indicating their motion statuses over frames. Finally, the motion status $M^i$ of an object instance $i$ across the scene is set as</p> \[M^{i} = \begin{cases} 1 \text{ if } \text{med}(\{m^i_n\}_n) \geq 0.5 \,, \\ 0 \text{ otherwise }\,, \end{cases}\] <p>where \(\text{med}(\{m^i_n\}_n)\) is the median of the motion labels for object $i$ in the sequence \(\{m^i_n\}_n\). If an instance object is labeled as 1, we denote this object as moving over the entire sequence.</p> <h3 id="pose-refinement">Pose Refinement</h3> <table><tr> <td> <img src="/assets/img/street/gt_noise_08.gif" alt="Drawing" style="width: 250px;"> </td> <td> <img src="/assets/img/street/gt_refined_08.gif" alt="Drawing" style="width: 250px;"> </td> </tr></table> <p><em><strong>Pose refinement results</strong>. Noise pose (left) and refined pose (right) of our results.</em></p> <p>To solve the aforementioned inaccurate camera pose problem, we jointly refine the camera poses with the point light field to account for these potential inaccuracies. We use the logarithmic representation of the rotation matrix such that the direction and the $l2$ norm of the rotation vector \(\boldsymbol{R} \in \mathbb{R}^{3}\) represents the axis and magnitude of rotation of the camera in the world-to-camera frame respectively. The translation vector \(\boldsymbol{t} \in \mathbb{R}^{3}\) represents the location of the camera in the world-to-camera frame.</p> <h3 id="self-supervised-training">Self-supervised Training</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/street/pipeline.png" sizes="95vw"></source> <img src="/assets/img/street/pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <strong>Pipeline&lt;/storng&gt; The pipeline of our method. Denote $$\boldsymbol{R^{\prime}}$$ as the set of rays that are cast from the camera center to the non-masked pixels only. This allows us to retain the information from static vehicles unlike previous masking-based approaches, which mask out all instances of commonly transient objects. Additionally, we reduce the uncertainty introduced by objects that are in motion, which is a very common feature of outdoor scenes. At inference time, we do not consider the mask and instead shoot rays through the entire pixel grid. Thus, the color $$C^{\prime}(\boldsymbol{r}_j)$$ of a ray $$\boldsymbol{r}_j$$ is given by $$ C^{\prime}(\boldsymbol{r}_j) = F_{\theta_{LF^{\prime}}}(\phi(\boldsymbol{d}_j) \oplus \phi(\boldsymbol{l}_j), \boldsymbol{R}^{\prime}, \boldsymbol{t}^{\prime}) $$ where $$\boldsymbol{d}_j$$ and $$\boldsymbol{l}_j$$ are the ray direction and the feature vector corresponding to $$\boldsymbol{r}_j$$, $$F_{\theta_{LF^{\prime}}}$$ is an MLP. The loss function is $$ \boldsymbol{L}_{m,r} = \sum_{j \in \boldsymbol{R^{\prime}}} || C^{\prime}(r_j) - C(r_j) ||^{2} $$ and the updates to the camera rotation and translation are optimized simultaneously with the neural point light field. ## Results We evaluate our method on the Waymo open dataset Waymo. We chose 6 scenes from Waymo which we believe are representative of street view scenes with different numbers of static and moving vehicles and pedestrians. We use the RGB images and the corresponding LiDAR point clouds for each scene. We drop out every 10th frame from the dataset for evaluation and train our method on the remaining frames. The RGB images are rescaled by a factor of 0.125 of their original resolutions for training. ### Novel view synthesis <video width="100%" autoplay="" muted="" loop=""> <source src="/assets/img/street/ours_recon_07.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> ### Trajectory extrapolation <video width="100%" autoplay="" muted="" loop=""> <source src="/assets/img/street/ours_07_exp.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> Our method uses point clouds as geometry priors. To prove that the network learns the actual scene geometry structure, instead of only learning the color appearance along the trained camera odometry, we extrapolate the trajectory to drift off from the training dataset. We then render views from this new trajectory which are far away from the training views. This differs from the novel view synthesis results presented in the previous paragraph where the network rendered views that were interpolated on the training trajectory. ## bibtex citation <figure class="highlight"><pre><code class="language-javascript" data-lang="javascript">    <span class="p">@</span><span class="nd">inproceedings</span><span class="p">{</span><span class="nx">deka2023erasing</span><span class="p">,</span>
    <span class="nx">title</span> <span class="o">=</span> <span class="p">{</span><span class="nx">Erasing</span> <span class="nx">the</span> <span class="na">Ephemeral</span><span class="p">:</span> <span class="nx">Joint</span> <span class="nx">Camera</span> <span class="nx">Refinement</span> <span class="nx">and</span> <span class="nx">Transient</span> <span class="nb">Object</span> <span class="nx">Removal</span> <span class="k">for</span> <span class="nx">Street</span> <span class="nx">View</span> <span class="nx">Synthesis</span><span class="p">},</span>
    <span class="nx">author</span> <span class="o">=</span> <span class="p">{</span><span class="nx">MS</span><span class="p">.</span> <span class="nx">Deka</span><span class="o">*</span> <span class="nx">and</span> <span class="nx">L</span><span class="p">.</span> <span class="nx">Sang</span><span class="o">*</span> <span class="nx">and</span> <span class="nx">Daniel</span> <span class="nx">Cremers</span><span class="p">},</span>
    <span class="nx">year</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2024</span><span class="p">},</span>
    <span class="nx">booktitle</span> <span class="o">=</span> <span class="p">{</span><span class="nx">GCPR</span><span class="p">},</span>
    <span class="nx">arxiv</span><span class="o">=</span><span class="p">{</span><span class="mf">2311.17634</span><span class="p">}</span>
    <span class="p">}</span>
 </code></pre></figure> </strong> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Lu Sang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>