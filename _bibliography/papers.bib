@inproceedings{sang2020wacv,
 title = {Inferring Super-Resolution Depth from a Moving Light-Source Enhanced RGB-D Sensor: A Variational Approach},
 abbr={WACV 2020},
 author = {L. Sang and B. Haefner and Daniel Cremers},
 booktitle = {WACV},
 year = {2020},
 additional_info = {  **(Spotlight Presentation)**},
 preview={inferring.png},
 bibtex_show={true},
 pdf={https://cvg.cit.tum.de/_media/spezial/bib/sang2020wacv.pdf},
 abstract={A novel approach towards depth map super-resolution using multi-view uncalibrated photometric stereo is presented. Practically, an LED light source is attached to a commodity RGB-D sensor and is used to capture objects
from multiple viewpoints with unknown motion. This nonstatic camera-to-object setup is described with a nonconvex variational approach such that no calibration on lighting or camera motion is required due to the formulation of an end-to-end joint optimization problem. Solving the proposed variational model results in high resolution depth, reflectance and camera pose estimates, as we show on challenging synthetic and real-world datasets.}
 }

@inproceedings{Sommer2022,
 author = {C. Sommer* and L. Sang* and D. Schubert and Daniel Cremers},
 abbr={CVPR 2022},
 title = {Gradient-{SDF}: {A} Semi-Implicit Surface Representation for 3D Reconstruction},
 booktitle = {CVPR},
 year = {2022},
 titleurl = {sommer2022.png},
 preview = {gradient_sdf.png},
 selected={true},
 bibtex_show={true},
 arxiv={2111.13652},
 abstract={Neural implicits have become popular for representing surfaces because they offer an adaptive resolution and support arbitrary topologies. While previous works rely on ground truth point clouds, they often ignore the effect of input quality and sampling methods during reconstructing. In this paper, we introduce a sampling method with an uncertainty-augmented surface implicit representation that employs a sampling technique that considers the geometric characteristics of inputs. To this end, we introduce a strategy that efficiently computes differentiable geometric features, namely, mean curvatures, to augment the sampling phase during the training period. The uncertainty augmentation offers insights into the occupancy and reliability of the output signed distance value, thereby expanding representation capabilities into open surfaces. Finally, we demonstrate that our method leads to state-of-the-art reconstructions on both synthetic and real-world data.}
}

@inproceedings{sang2023high,
 author = {L. Sang and B. Haefner and Xingxing Zuo and Daniel Cremers},
 title = {High-Quality RGB-D Reconstruction via Multi-View Uncalibrated Photometric Stereo and Gradient-SDF},
 abbr={WACV 2023},
 booktitle = {WACV},
 year = {2023},
 additional_info = {  **(Spotlight Presentation)**},
 preview={high_rgbd.png},
 selected={true},
 bibtex_show={true},
 arxiv={2210.12202},
 abstract={Fine-detailed reconstructions are in high demand in many applications. However, most of the existing RGB-D reconstruction methods rely on pre-calculated accurate camera poses to recover the detailed surface geometry, where the representation of a surface needs to be adapted when optimizing different quantities. In this paper, we present a novel multi-view RGB-D based reconstruction method that tackles camera pose, lighting, albedo, and surface normal estimation via the utilization of a gradient signed distance field gradient-SDF. The proposed method formulates the image rendering process using specific physically-based models and optimizes the surface's quantities on the actual surface using its volumetric representation, as opposed to other works which estimate surface quantities only near the actual surface. To validate our method, we investigate two physically-based image formation models for natural light and point light source applications. The experimental results on synthetic and real-world datasets demonstrate that the proposed method can recover high-quality geometry of the surface more faithfully than the state-of-the-art and further improves the accuracy of estimated camera poses.},
 website={https://sangluisme.github.io/projects/4_project/}
}

@inproceedings{sang2023enhanching,
 title = {Enhancing Surface Neural Implicits with Curvature-Guided Sampling and Uncertainty-Augmented Representations},
 abbr={ECCV 2024},
 author = {L. Sang and Ahbishek Saroha and Maolin Gao and Daniel Cremers},
 booktitle = {ECCVW},
 year = {2024},
 selected={true},
 preview={curvature.png},
 bibtex_show={true},
 arxiv={2306.02099},
 abstract={Neural implicits have become popular for representing surfaces because they offer an adaptive resolution and support arbitrary topologies. While previous works rely on ground truth point clouds, they often ignore the effect of input quality and sampling methods during reconstructing. In this paper, we introduce a sampling method with an uncertainty-augmented surface implicit representation that employs a sampling technique that considers the geometric characteristics of inputs. To this end, we introduce a strategy that efficiently computes differentiable geometric features, namely, mean curvatures, to augment the sampling phase during the training period. The uncertainty augmentation offers insights into the occupancy and reliability of the output signed distance value, thereby expanding representation capabilities into open surfaces. Finally, we demonstrate that our method leads to state-of-the-art reconstructions on both synthetic and real-world data.},
 website={https://sangluisme.github.io/projects/3_project/}
}

@inproceedings{deka2023erasing,
 title = {Erasing the Ephemeral: Joint Camera Refinement and Transient Object Removal for Street View Synthesis},
 author = {MS. Deka* and L. Sang* and Daniel Cremers},
 year = {2024},
 selected={true},
 abbr={GCPR 2024},
 booktitle = {GCPR},
 preview={street.png},
 bibtex_show={true},
 arxiv={2311.17634},
 abstract={Synthesizing novel views for urban environments is crucial for tasks like autonomous driving and virtual tours. Compared to object-level or indoor situations, outdoor settings present unique challenges such as inconsistency across frames due to moving vehicles and camera pose drift over lengthy sequences. In this paper, we introduce a method that tackles these challenges on view synthesis for outdoor scenarios. We employ a neural point light field scene representation and strategically detect and mask out dynamic objects to reconstruct novel scenes without artifacts. Moreover, we simultaneously optimize camera pose along with the view synthesis process, and thus we simultaneously refine both elements. Through validation on real-world urban datasets, we demonstrate state-of-the-art results in synthesizing novel views of urban scenes.},
 website={https://sangluisme.github.io/projects/2_project/}
}

@inproceedings{komorowicz2023coloring,
 title = {Coloring the Past: Neural Historical Buildings Reconstruction from Archival Photography},
 author = {David Komorowicz* and Lu Sang* and Ferdinand Maiwald and Daniel Cremers},
 abbr={ECCV 2024},
 year = {2024},
 selected={true},
 booktitle = {ECCVW},
preview={historical.png},
bibtex_show={true},
 arxiv={2311.17810},
 abstract={Historical buildings are a treasure and milestone of human cultural heritage. Reconstructing the 3D models of these building hold significant value. The rapid development of neural rendering methods makes it possible to recover the 3D shape only based on archival photographs. However, this task presents considerable challenges due to the limitations of such datasets. Historical photographs are often limited in number and the scenes in these photos might have altered over time. The radiometric quality of these images is also often sub-optimal. To address these challenges, we introduce an approach to reconstruct the geometry of historical buildings, employing volumetric rendering techniques. We leverage dense point clouds as a geometric prior and introduce a color appearance embedding loss to recover the color of the building given limited available color images. We aim for our work to spark increased interest and focus on preserving historical buildings. Thus, we also introduce a new historical dataset of the Hungarian National Theater, providing a new benchmark for the reconstruction method.},
 website={https://sangluisme.github.io/projects/1_project/}
}

@inproceedings{haerenstam2024diffcd,
 title = {DiffCD: A Symmetric Differentiable Chamfer Distance for Neural Implicit Surface Fitting},
 author = {L. HÃ¤renstam-Nielsen and Lu Sang and Ahbisheck Saroha and N. Araslanov and D. Cremers},
 abbr={ECCV 2024},
 booktitle = {ECCV},
 year = {2024},
 preview={diffcd.png},
 selected={true},
 bibtex_show={true},
 arxiv={2407.17058},
 abstract={Fitting neural implicit surfaces to point clouds is typically done by encouraging the network output to equal zero on the point cloud. Yet, since the underlying shape metric is not symmetric, previous methods are susceptible to spurious surfaces. We theoretically analyze the predominant approach for dealing with spurious surfaces, and show that it is equivalent to regularizing the surface area, leading to over-smoothing. We propose a novel loss function corresponding to the symmetric Chamfer distance to address these shortcomings. It assures both that the points are near the surface and that the surface is near the points. Our approach reliably recovers a high level of shape detail and eliminates spurious surfaces without the need for additional regularization. To make our approach more practical, we further propose an efficient method for uniformly sampling point batches from the implicit surface.}
}

